{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "* You need to code in this jupyter notebook only.\n",
    "* Download this notebook and import in your jupyter lab.\n",
    "* You need to write a partial code for step 0 to step 8 mentioned with prefix ##\n",
    "* Fill the blanks where it is instructed in comments. \n",
    "* Leave other codes, structure as it is.\n",
    "* Follow all the instructions commented in a cells.\n",
    "\n",
    "\n",
    "\n",
    "**Answer the questions given at the end of this notebook within your report.**\n",
    "\n",
    "**Upload this jupyter notebook after completion with your partial code and the report in one file in PDF format.**\n",
    "\n",
    "**Also upload the resulting image showing all the selected points and boundary line between them after LDA analysis.**\n",
    "\n",
    "**Your file name should be yourname_lab4.pdf. Upload it before the due time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np ## import numpy\n",
    "import cv2 ## import opencv\n",
    "import matplotlib ## import matplotlib\n",
    "import matplotlib.pyplot as plt ## import matplotlib pyplot\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis   ## from sklearn import LDA analysis\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "##---------------------------------------------------\n",
    "## Step 0: Install all other dependencies that occur at run time if  any module not found.\n",
    "##---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_points = 25  ## Number of points you want select from each strip. Recommended >= 20 \n",
    "\n",
    "img = cv2.imread('Indian_Flag.jpg') ## Read the given image\n",
    "\n",
    "def select_points(img, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    #------------------------------------------\n",
    "    ## step 1: Convert the img from BGR to RGB using cv2 and display it using cv2.imshow\n",
    "    RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(RGB_img)\n",
    "    ## step 2: Put title of the image\n",
    "    ax.set_title(title)\n",
    "    ##-----------------------------------------\n",
    "    \n",
    "    # Set the cursor style to a plus sign\n",
    "    fig.canvas.manager.set_window_title('Select Points')\n",
    "    cursor = matplotlib.widgets.Cursor(ax, useblit=True, color='red', linewidth=1)\n",
    "    plt.show(block=False)  # Show the image without blocking\n",
    "\n",
    "    k = 0\n",
    "    points = [] ## Create here an empty list to store points \n",
    "\n",
    "    while k < Number_of_points:\n",
    "        xy = plt.ginput(1, timeout=0)  # Non-blocking input\n",
    "        if len(xy) > 0:\n",
    "            col, row = map(int, xy[0])  # Convert to integer\n",
    "            ##-----------------------------------------------\n",
    "            ## Step 3: Collect RGB values at the clicked positions (col, row) and print it. \n",
    "            RGB_val = RGB_img[row][col]\n",
    "            print(RGB_val)\n",
    "            ##-----------------------------------------------\n",
    "\n",
    "            k += 1\n",
    "            points.append([row, col, img[row, col]])  # Store RGB values in empty list points.\n",
    "            \n",
    "            # Display colored dot on the image\n",
    "            plt.scatter(col, row, c='black', marker='o', s=10)\n",
    "\n",
    "            # Redraw the image to include the dot\n",
    "            plt.draw()\n",
    "\n",
    "    plt.close()  # Close the window after all points are collected\n",
    "    return points ## Fill this blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248  88  26]\n",
      "[250  96  26]\n",
      "[252  91  23]\n",
      "[252  91  23]\n",
      "[250  89  21]\n",
      "[246  79   8]\n",
      "[246  82  11]\n",
      "[248  81  10]\n",
      "[251  84  14]\n",
      "[242  78  17]\n",
      "[243  85  12]\n",
      "[246  89  18]\n",
      "[254  93  23]\n",
      "[246  88  23]\n",
      "[249  92  21]\n",
      "[248  91  20]\n",
      "[243  70   1]\n",
      "[249  82  14]\n",
      "[245  78   7]\n",
      "[238  70   0]\n",
      "[239  81  20]\n",
      "[252  91  13]\n",
      "[248  82   8]\n",
      "[245  86  20]\n",
      "[249  88  18]\n",
      "[211 211 219]\n",
      "[218 209 226]\n",
      "[215 224 233]\n",
      "[213 222 219]\n",
      "[224 221 232]\n",
      "[221 220 226]\n",
      "[220 218 231]\n",
      "[224 221 230]\n",
      "[226 225 243]\n",
      "[219 217 220]\n",
      "[211 208 227]\n",
      "[222 219 238]\n",
      "[219 220 240]\n",
      "[233 232 246]\n",
      "[226 227 245]\n",
      "[226 227 245]\n",
      "[224 212 234]\n",
      "[224 212 234]\n",
      "[222 223 228]\n",
      "[227 224 231]\n",
      "[227 227 235]\n",
      "[226 218 242]\n",
      "[214 212 226]\n",
      "[221 218 237]\n",
      "[222 220 234]\n",
      "[ 41 103  78]\n",
      "[34 98 72]\n",
      "[31 99 74]\n",
      "[28 98 70]\n",
      "[ 30 100  72]\n",
      "[ 30 103  74]\n",
      "[ 38 106  81]\n",
      "[ 33 103  77]\n",
      "[27 91 64]\n",
      "[27 98 68]\n",
      "[33 98 74]\n",
      "[ 29 102  75]\n",
      "[26 96 68]\n",
      "[29 99 73]\n",
      "[33 96 67]\n",
      "[30 95 65]\n",
      "[ 29 100  70]\n",
      "[22 92 66]\n",
      "[31 96 72]\n",
      "[30 96 69]\n",
      "[28 95 64]\n",
      "[29 93 66]\n",
      "[32 98 71]\n",
      "[ 34 103  75]\n",
      "[27 93 66]\n"
     ]
    }
   ],
   "source": [
    "##-----------------------------------------------------------------\n",
    "## Step4: fill the blanks for Selected points from saffron strip\n",
    "pts_saffron = select_points(img, 'Saffron')\n",
    "## Step5: fill the blanks for Selected points from white strip)\n",
    "pts_white = select_points(img, 'White')\n",
    "## Step6: fill the blanks for Selected points from green strip\n",
    "pts_green = select_points(img, 'Green')\n",
    "##-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RGB values to Lab color space\n",
    "def rgb_to_lab(rgb):\n",
    "    return cv2.cvtColor(np.uint8([[rgb]]), cv2.COLOR_RGB2Lab)[0][0]\n",
    "\n",
    "saffron_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_saffron])\n",
    "white_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_white])\n",
    "green_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_green])\n",
    "\n",
    "## Step7: Extract a* and b* components from Lab color space\n",
    "a_features = np.hstack((saffron_lab[:, 1], white_lab[:, 1], green_lab[:, 1]))\n",
    "b_features = np.hstack((saffron_lab[:, 2], white_lab[:, 2], green_lab[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class labels to numeric values\n",
    "class_mapping = {'Saffron': 0, 'White': 1, 'Green': 2}\n",
    "y = np.array([class_mapping[label] for label in ['Saffron'] * Number_of_points + ['White'] * Number_of_points + ['Green'] * Number_of_points])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(a_features[:Number_of_points], b_features[:Number_of_points], c='b', marker='o', s=50, label='Saffron')\n",
    "plt.scatter(a_features[Number_of_points:2*Number_of_points], b_features[Number_of_points:2*Number_of_points], c='g', marker='^', s=50, label='White')\n",
    "plt.scatter(a_features[2*Number_of_points:], b_features[2*Number_of_points:], c='r', marker='*', s=50, label='Green')\n",
    "plt.legend(['Saffron', 'White', 'Green'], loc='best')\n",
    "plt.xlabel('Red-Green (a*)')  ## Provide x label\n",
    "plt.ylabel('Blue-Yellow (b*)') ## Provide y label\n",
    "plt.title('Colour Distribution') ## Provide title\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('Plotted_Points.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "##------------------------------------------------------------\n",
    "# Step 8: Perform LDA analysis using LinearDiscriminantAnalysis() and lda.fit()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X = np.column_stack((a_features, b_features))\n",
    "lda.fit(X, y)  \n",
    "\n",
    "##-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LDA boundaries\n",
    "plt.figure()\n",
    "plt.scatter(a_features[:Number_of_points], b_features[:Number_of_points], c='b', marker='o', s=50, label='Saffron')\n",
    "plt.scatter(a_features[Number_of_points:2*Number_of_points], b_features[Number_of_points:2*Number_of_points], c='g', marker='^', s=50, label='White')\n",
    "plt.scatter(a_features[2*Number_of_points:], b_features[2*Number_of_points:], c='r', marker='*', s=50, label='Green')\n",
    "\n",
    "plt.xlabel('Red-Green (a*)')  ## Provide x label\n",
    "plt.ylabel('Blue-Yellow (b*)') ## Provide y label\n",
    "plt.title('LDA boundaries (linear model) for Colors of the Indian Flag')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, colors='k', linewidths=2, linestyles='solid')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('Plotted_LDA_Boundaries', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report:\n",
    "\n",
    "## Answer the following questions within your report:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tWhat are the key assumptions underlying LDA, and how do these assumptions influence the model's performance?\n",
    "Ans1: Assumptions of LDA are **Normality, Homoscedasticity, Linearity and Independence**. LDA assumes that the data with each class should follow a normal/gaussian distribution (Normality), that covariance matrices of different classes are equal (Homoscedasticity) and that the relationship between target classes and predictor variables is linear ie, a linear decision boundary should be sufficient to separate different classes (Linearity). Furthermore, LDA assumes features are not highly correlated ie no multicollinearity (Independence of features). Violation of Normality/Linearity **affects the model's ability to accurately separate** skewed/multiodal/non-linear classes, multicollinearity makes the within class matrix become singular, making it difficult to calculate the inverse of it and violation of homoscedasticity can lead to non-linear decision boundaries as it affects Mahalanobis distance. \n",
    "\n",
    "### 2.\tWhat are the hyperparameters in LDA, and how do they affect the outcome of the model?\n",
    "Some of the key parameters in LDA are **solvers, shrinkage and number of components**. Different solvers like svd, lsqr and eigen are offered by LDA implementations. The effect that they have on the outcome of the model is in terms of computational speed and numerical stability for some cases. Shrinkage (only lsqr and eigen) regularises the covariance matrices by adding a small value to their diagonals. This affects outcomes by improving the stability of covariance matrices, which can prevent overfitting and generalization performance overall. Number of components determines the number of new dimensions the LDA will create and directly influences model performance. If n is too small then you may lose out on important information and otherwise if it is too large then it might lead to overfitting and you lose out on the potential to reduce dimensions. Other paramters include priors(specifies prior probabilties for each class) and tolerance (stopping criteria). \n",
    "\n",
    "### 3.\tWhat methods can be used to assess an LDA model's effectiveness in terms of separation of topics and the coherence of generated topics?\n",
    "There are various methods that can be used to asses and LDA model's effectiveness in terms of separation of topics. **Scatter plots** of LDA components can be visualised to show how effectively LDA separates different classes. **Explained Vraiance ratio**, indicating how much discriminative information is retained by each component and **mahalanobis dist.** which measures degree of class separation are other useful measures. Classification performance in general can be measured through **accuracy, precision, recall, ROC Curves and AUC curves**, etc. As for coherence of generated topics, metrics assessing semantic similarity and co-occurence of words like *C_v, UMass or C_uci* can be used. \n",
    "\n",
    "### 4.\tWhat are some common challenges or limitations associated with LDA, and how can they be addressed or mitigated?\n",
    "LDA has several limitations. Since it relies on mean and covariances estimates, it is very **sensitive to outliers** and is even not the best at dealing with **imbalanced data** and can lead to biased classification for dominating classes. Further, LDA can also only reduce feature space to **at most C-1 dimensions** leading to loss of some high-dimensional data. Thirdly, it **cannot separate non-linear data**, making it inefficient when dealing with complex patterns in data. \n",
    "\n",
    "How can they be addressed:\n",
    "We may deal with outliers before hand by applying methods like z-score filtering, IQR methods or even Regularised LDA. For more dimensions, we can use CA before LDA to retain variance and we may also apply kernels to extend LDA to higher dimensional spaces for the same. In this way, Kernels can also help non-linear classes become separable. We can address dominant classes by adjusting priors as well. \n",
    "\n",
    "### 5. What practical applications does this assignment have in real-world situations, and what benefits does it offer in those specific scenarios?\n",
    "This assignment on color classification has a lot of real-world uses. It helps industries check for defects in fabrics, determine if fruit is ripe, and improve image segmentation for medical scans and self-driving cars. In dermatology, it’s used to track skin conditions over time, and in environmental monitoring, it helps analyze satellite images to detect pollution and land changes. The main benefits are **better accuracy, automation, and efficiency—cutting down human error, making processes faster, and improving decision-making in different fields**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
